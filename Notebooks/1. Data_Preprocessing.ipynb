{"cells":[{"cell_type":"markdown","source":["# **Data Preprocessing**\n","\n","\n"],"metadata":{"id":"JO2QQTMexa6m"},"id":"JO2QQTMexa6m"},{"cell_type":"markdown","source":["## 1. Python packages"],"metadata":{"id":"8pt0ofUcxZ73"},"id":"8pt0ofUcxZ73"},{"cell_type":"code","execution_count":1,"metadata":{"id":"f6202133-f05c-4ef1-9b60-d65d0098bea1","executionInfo":{"status":"ok","timestamp":1718980959961,"user_tz":-120,"elapsed":1142,"user":{"displayName":"Daan Mulckhuyse","userId":"18102877053212470272"}}},"outputs":[],"source":["# Load neccesary Python packages:\n","import pandas as pd\n","import os\n","import shutil\n","import numpy as np\n","import tqdm\n","import librosa\n","import ntpath\n","import soundfile as sf\n","import glob"],"id":"f6202133-f05c-4ef1-9b60-d65d0098bea1"},{"cell_type":"markdown","metadata":{"id":"fdcdd57c-b33f-4541-9cfd-3fb78b6ac1df"},"source":["## 2. Splitting the data\n"],"id":"fdcdd57c-b33f-4541-9cfd-3fb78b6ac1df"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"f08aadd2-4e3b-4bf1-980c-1b3a3aa276e7"},"outputs":[],"source":["# Split the data in the predefined train, validation, and test groups:\n","\n","# Directory to the complete dataset:\n","data_dir = ''\n","\n","# Target directory for the splits:\n","target_base_dir = ''\n","\n","# Load the csv file attached to the InsectSet66 dataset:\n","data_metadata = pd.read_csv('...../InsectSet66_Train_Val_Test_Annotation.csv')\n","\n","# Select all audio file names in the InsectSet66 dataset:\n","files = os.listdir(data_dir)\n","\n","\n","# Iterate through the filenames to copy them to their split directory:\n","for File in files:\n","\n","    # Match every filename to a row in the csv file:\n","    result_row = data_metadata.loc[data_metadata['file_name'] == File]\n","\n","    # Check if there was a match:\n","    if not result_row.empty:\n","\n","        # Retrieve the predefined group for this split:\n","        subset = result_row['subset'].values[0]\n","\n","        # Define target directory based on the subset:\n","        target_dir = os.path.join(target_base_dir, subset.capitalize())\n","\n","        # Make sure the target directory exists:\n","        if not os.path.exists(target_dir):\n","            os.makedirs(target_dir)\n","\n","        source_file_path = os.path.join(data_dir, File)  # Source file path.\n","        target_file_path = os.path.join(target_dir, File)  # Target file path.\n","\n","        # Copy the file to the target directory (Consider moving instead of copying when dealing with limited storage capacity):\n","        shutil.copy(source_file_path, target_file_path)\n","        # shutil.move(source_file_path, target_file_path)\n"],"id":"f08aadd2-4e3b-4bf1-980c-1b3a3aa276e7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c10008ce-64d6-41f7-b341-6ebfbd607fac"},"outputs":[],"source":["# Check if splitting was succesful:\n","\n","# Load the directories for the original and new file locations:\n","# Load the csv file attached to the InsectSet66 dataset:\n","data_metadata = pd.read_csv('...../InsectSet66_Train_Val_Test_Annotation.csv')\n","\n","# Directory of data splits:\n","data_dir = ''\n","\n","# Names of the maps containing the split data:\n","maps = ['Train', 'Validation', 'Test']\n","\n","# Get counts from the csv file:\n","count_metadata = data_metadata['subset'].value_counts()\n","\n","# Initialize a dictionary to hold the counts:\n","counts_dict = {\n","    'Category': [],\n","    'Metadata_Count': [],\n","    'Actual_Count': []\n","  }\n","\n","# Fill the dictionary:\n","for category in maps:\n","\n","    normalized_category = category.lower()\n","\n","    # Add the category name:\n","    counts_dict['Category'].append(category)\n","\n","    # Add the metadata count:\n","    metadata_count = count_metadata.get(normalized_category, 0)\n","    counts_dict['Metadata_Count'].append(metadata_count)\n","\n","    # Add actual count from filesystem:\n","    folder_path = os.path.join(data_dir, category)\n","    actual_count = len(os.listdir(folder_path))\n","    counts_dict['Actual_Count'].append(actual_count)\n","\n","# Create a DataFrame from the dictionary:\n","comparison_df = pd.DataFrame(counts_dict)\n","\n","# Check if the values in the dataframe match:\n","print(comparison_df)\n"],"id":"c10008ce-64d6-41f7-b341-6ebfbd607fac"},{"cell_type":"code","source":["# Use this code to find missing files when values in the dataframe do not match:\n","\n","# Load the csv file attached to the InsectSet66 dataset:\n","data_metadata = pd.read_csv('...../InsectSet66_Train_Val_Test_Annotation.csv')\n","\n","# Directory of data splits:\n","data_dir = ''\n","\n","# Names of the maps containing the split data:\n","maps = ['Train', 'Validation', 'Test']\n","\n","# Initialize a dictionary to store filenames:\n","directory_files = {}\n","\n","# Reading file names from directories:\n","for category in maps:\n","\n","    folder_path = os.path.join(data_dir, category)\n","\n","    # List files in the directory, only consider files (ignore subdirectories):\n","    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n","    directory_files[category.lower()] = set(files)\n","\n","# Check for discrepancies:\n","for category in maps:\n","    normalized_category = category.lower()\n","\n","    # Get file names from metadata for the current category:\n","    metadata_files = set(data_metadata[data_metadata['subset'] == normalized_category]['file_name'])\n","\n","    # Files in metadata but not in directory:\n","    missing_files = metadata_files - directory_files[normalized_category]\n","    if missing_files:\n","        print(f'{len(missing_files)} Files in metadata but not in {category} directory:', missing_files)\n","    else:\n","        print(f'All files in {category} category are accounted for in the directory.')\n"],"metadata":{"id":"sBKY6K1YwVY5"},"id":"sBKY6K1YwVY5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check if none of the files got corrupted while moving them to different directories:\n","\n","# Directory of data splits:\n","data_dir = ''\n","\n","# Names of the maps containing the split data:\n","maps = ['Train', 'Validation', 'Test']\n","\n","# Initialize count:\n","count = 0\n","\n","# Walk through all files:\n","for subset in maps:\n","  directory = os.join.path(data_dir, subset)\n","  for root, dirs, files in os.walk(directory):\n","      for file in files:\n","\n","          # Get the path to the file:\n","          filepath = os.path.join(root, file)\n","\n","          # Check if the size of the file is 0 using os.path.getsize:\n","          if os.path.getsize(filepath) == 0:\n","\n","              # Print the path to the file:\n","              count += 1\n","              print('0 KB file found:', filepath)\n","\n","  if count == 0:\n","    print(f'No corrupted files in {subset}:)')\n"],"metadata":{"id":"g7_72bmOwXbI"},"id":"g7_72bmOwXbI","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Standardizing the data"],"metadata":{"id":"D3o60sb1vqun"},"id":"D3o60sb1vqun"},{"cell_type":"markdown","source":["### 3.1 Custom functions"],"metadata":{"id":"wOQiIbwAzS7a"},"id":"wOQiIbwAzS7a"},{"cell_type":"markdown","source":["All custom functions are slightly modified versions of Faiss' (2023) preprocessing script.\n","\n","https://github.com/mariusfaiss/InsectSet47-InsectSet66-Adaptive-Representations-of-Sound-for-Automatic-Insect-Recognition/blob/main/SplitAudioChunks.py\n"],"metadata":{"id":"FcPXO3kRzaLI"},"id":"FcPXO3kRzaLI"},{"cell_type":"code","source":["def preprocess_audio(wave_paths, sample_rate, sample_buffer, out_path):\n","    \"\"\"\n","    Main file to preprocess a list of audio files.\n","    Audio files longer than sample_buffer are chunked into small\n","    overlapping fixed size windows.\n","    Audio files shorter than sample_buffer are padded with zeros or\n","    looped to sample_buffer size, respectively.\n","\n","    Args:\n","        wave_paths: list, filepaths to audio files to be preprocessed\n","        sample_rate: int, sample rate of audio files\n","        sample_buffer: float, total sample buffer length. Calculated as window_size*sample_rate.\n","        out_path: str, output path for saving.\n","    \"\"\"\n","\n","    for filename in tqdm(wave_paths):\n","        audio, _ = librosa.load(filename, sr=sample_rate)\n","        file_length = librosa.get_duration(y=audio, sr=sample_rate)\n","        name = ntpath.basename(filename[:-4])\n","\n","        samples_total = file_length * sample_rate\n","\n","        if samples_total < sample_buffer:\n","            pad_short(audio, sample_rate, sample_buffer, samples_total, out_path, name)\n","            loop_short(audio, sample_rate, sample_buffer, samples_total, out_path, name)\n","        elif file_length >= chunk_length:\n","            chunk_long(audio, sample_rate, sample_buffer, samples_total, out_path, name)\n"],"metadata":{"id":"F91sbnjZy3sn"},"id":"F91sbnjZy3sn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loop_short(audio, sample_rate, sample_buffer, samples_total, out_path, name):\n","        \"\"\"\n","        Loop short audio files until the sample_buffer length is reached.\n","\n","        Args:\n","            audio: array, audio waveform.\n","            sample_rate: int, sample rate of audio files.\n","            sample_buffer: float, total sample buffer length.Calculated as window_size*sample_rate.\n","            samples_total: int, total number of samples for calculating the amount of loops.\n","            out_path: str, output path for saving.\n","            name: str, name of the audio file.\n","        \"\"\"\n","\n","        count = int(sample_buffer / samples_total) + (sample_buffer % samples_total > 0)\n","        i = 1\n","        loop = audio\n","\n","        while i < count:\n","            loop = np.concatenate([loop, audio])\n","            i += 1\n","\n","        loop = loop[: int(sample_buffer)]\n","        sf.write(f'{out_path+name}_loop.wav', loop, sample_rate)\n"],"metadata":{"id":"20tm-F-hy6Ia"},"id":"20tm-F-hy6Ia","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pad_short(audio, sample_rate, sample_buffer, samples_total, out_path, name):\n","        \"\"\"\n","        Pad short audio files until the sample_buffer length is reached.\n","\n","        Args:\n","            audio: array, audio waveform.\n","            sample_rate: int, sample rate of audio files.\n","            sample_buffer: float, total sample buffer length. Calculated as window_size*sample_rate.\n","            samples_total: int, total number of samples for calculating the amount of loops.\n","            out_path: str, output path for saving.\n","            name: str, name of the audio file.\n","        \"\"\"\n","\n","        pad = int(sample_buffer - samples_total)\n","        wave = np.pad(audio, (0, pad))\n","\n","        sf.write(f'{out_path+name}_padded.wav', wave, sample_rate)\n"],"metadata":{"id":"uvziZ04sy76b"},"id":"uvziZ04sy76b","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1NAqUbfIlKVI"},"outputs":[],"source":["def chunk_long(audio, sample_rate, sample_buffer, samples_total, out_path, name):\n","        \"\"\"\n","        Chunk audio files into small overlapping fixed size windows.\n","        End chunks are wrapped.\n","\n","        Args:\n","            audio: array, audio waveform.\n","            sample_rate: int, sample rate of audio files.\n","            sample_buffer: float, total sample buffer length. Calculated as window_size*sample_rate.\n","            samples_total: int, total number of samples for calculating the amount of loops.\n","            out_path: str, output path for saving.\n","            name: str, name of the audio file.\n","        \"\"\"\n","\n","        samples_wrote = 0\n","        counter = 1\n","        while samples_wrote < samples_total:\n","            if (samples_total - samples_wrote) >= sample_buffer:\n","                chunk = audio[samples_wrote: int(samples_wrote + sample_buffer)]\n","                sf.write(f'{out_path+name}_chunk{counter}.wav', chunk, sample_rate)\n","                samples_wrote = int(samples_wrote + sample_buffer - overlap_samples)\n","                counter += 1\n","\n","            # Wrap audio for end chunks:\n","            if (samples_total - samples_wrote) < sample_buffer:\n","                if (samples_total - samples_wrote) > min_samples:\n","                    wrap_length = int(sample_buffer - (samples_total - samples_wrote))\n","                    wrap = audio[0: int(wrap_length)]\n","                    chunk = audio[samples_wrote: int(samples_wrote + sample_buffer)]\n","                    wrapped_file = np.concatenate([chunk, wrap])\n","                    sf.write(f'{out_path+name}_wrap{counter}.wav', wrapped_file, sample_rate)\n","                    counter += 1\n","                samples_wrote = int(samples_wrote + sample_buffer - overlap_samples)\n"],"id":"1NAqUbfIlKVI"},{"cell_type":"markdown","source":["### 3.2 Model 1"],"metadata":{"id":"nBc3YuJhysaY"},"id":"nBc3YuJhysaY"},{"cell_type":"code","execution_count":null,"metadata":{"id":"xtIHLZepB_z0"},"outputs":[],"source":["# Define (window length, window overlap):\n","sample_rate = 44100\n","chunk_length = 5\n","chunk_overlap = 2.5\n","min_length = 1.25\n","\n","# calculate global variables:\n","sample_buffer = chunk_length * sample_rate         # Number of samples per chunk.\n","overlap_samples = chunk_overlap * sample_rate      # Overlap of chunks in samples.\n","min_samples = min_length * sample_rate             # Minimum end samples.\n","\n","# Name of maps containing the splits:\n","dsets = ['Train', 'Val', 'Test']\n","\n","# Directory to output standardized fragments:\n","outdir = f''\n","\n","# Create a new dictionary for saving the standardized fragments:\n","os.makedirs(outdir, exist_ok=False)\n","\n","# Iterate through the maps:\n","for ds in dsets:\n","    os.makedirs(f'{outdir}/{ds}', exist_ok=False)\n","    paths = glob(f'/content/drive/MyDrive/Thesis/{ds}/*.wav')\n","    out_path = f'{outdir}/{ds}/'\n","    preprocess_audio(paths, sample_rate, sample_buffer, out_path)\n","\n","\n","# Load the csv file attached to the InsectSet66 dataset:\n","df = pd.read_csv('...../InsectSet66_Train_Val_Test_Annotation.csv')\n","df = df[['file_name', 'unique_file', 'path', 'label', 'subset']]\n","\n","all_dfs = []\n","for i in tqdm(range(len(df))):\n","    name = ntpath.basename(df.iloc[i]['path'][:-4])\n","    subset = df.iloc[i]['subset']\n","    subset = 'val' if subset == 'validation' else subset\n","    chunks = glob(f'{outdir}/{subset}/{name}*.wav')\n","    n_chunks = len(chunks)\n","    new_df = pd.DataFrame(np.tile(df.iloc[i].values, n_chunks).reshape(n_chunks, len(df.columns)) , columns=df.columns)\n","    new_df['path'] = chunks\n","    all_dfs.append(new_df)\n","\n","pp_df = pd.concat(all_dfs)\n","pp_df.to_csv(f'{outdir}/metadata.csv', index=False)"],"id":"xtIHLZepB_z0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1acYDgPRoJLz"},"outputs":[],"source":["# Check if none of the files got corrupted while moving them to different directories:\n","\n","# Directory of data splits:\n","data_dir = ''\n","\n","# Names of the maps containing the split data:\n","maps = ['Train', 'Validation', 'Test']\n","\n","# Initialize count:\n","count = 0\n","\n","# Walk through all files:\n","for subset in maps:\n","  directory = os.join.path(data_dir, subset)\n","  for root, dirs, files in os.walk(directory):\n","      for file in files:\n","\n","          # Get the path to the file:\n","          filepath = os.path.join(root, file)\n","\n","          # Check if the size of the file is 0 using os.path.getsize:\n","          if os.path.getsize(filepath) == 0:\n","\n","              # Print the path to the file:\n","              count += 1\n","              print('0 KB file found:', filepath)\n","\n","  if count == 0:\n","    print(f'No corrupted files in {subset}:)')\n"],"id":"1acYDgPRoJLz"},{"cell_type":"markdown","source":["### 3.3 Model 2"],"metadata":{"id":"PxrvMEeIyvks"},"id":"PxrvMEeIyvks"},{"cell_type":"code","source":["# Define (window length, window overlap):\n","sample_rate = 44100\n","chunk_length = 5\n","chunk_overlap = 3.75\n","min_length = 1.25\n","\n","# calculate global variables:\n","sample_buffer = chunk_length * sample_rate         # Number of samples per chunk.\n","overlap_samples = chunk_overlap * sample_rate      # Overlap of chunks in samples.\n","min_samples = min_length * sample_rate             # Minimum end samples.\n","\n","# Name of maps containing the splits:\n","dsets = ['Train', 'Val', 'Test']\n","\n","# Directory to output standardized fragments:\n","outdir = f''\n","\n","# Create a new dictionary for saving the standardized fragments:\n","os.makedirs(outdir, exist_ok=False)\n","\n","# Iterate through the maps:\n","for ds in dsets:\n","    os.makedirs(f'{outdir}/{ds}', exist_ok=False)\n","    paths = glob(f'/content/drive/MyDrive/Thesis/{ds}/*.wav')\n","    out_path = f'{outdir}/{ds}/'\n","    preprocess_audio(paths, sample_rate, sample_buffer, out_path)\n","\n","\n","# Load the csv file attached to the InsectSet66 dataset:\n","df = pd.read_csv('...../InsectSet66_Train_Val_Test_Annotation.csv')\n","df = df[['file_name', 'unique_file', 'path', 'label', 'subset']]\n","\n","all_dfs = []\n","for i in tqdm(range(len(df))):\n","    name = ntpath.basename(df.iloc[i]['path'][:-4])\n","    subset = df.iloc[i]['subset']\n","    subset = 'val' if subset == 'validation' else subset\n","    chunks = glob(f'{outdir}/{subset}/{name}*.wav')\n","    n_chunks = len(chunks)\n","    new_df = pd.DataFrame(np.tile(df.iloc[i].values, n_chunks).reshape(n_chunks, len(df.columns)) , columns=df.columns)\n","    new_df['path'] = chunks\n","    all_dfs.append(new_df)\n","\n","pp_df = pd.concat(all_dfs)\n","pp_df.to_csv(f'{outdir}/metadata.csv', index=False)"],"metadata":{"id":"G1TypvA-4ZhY"},"id":"G1TypvA-4ZhY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check if none of the files got corrupted while moving them to different directories:\n","\n","# Directory of data splits:\n","data_dir = ''\n","\n","# Names of the maps containing the split data:\n","maps = ['Train', 'Validation', 'Test']\n","\n","# Initialize count:\n","count = 0\n","\n","# Walk through all files:\n","for subset in maps:\n","  directory = os.join.path(data_dir, subset)\n","  for root, dirs, files in os.walk(directory):\n","      for file in files:\n","\n","          # Get the path to the file:\n","          filepath = os.path.join(root, file)\n","\n","          # Check if the size of the file is 0 using os.path.getsize:\n","          if os.path.getsize(filepath) == 0:\n","\n","              # Print the path to the file:\n","              count += 1\n","              print('0 KB file found:', filepath)\n","\n","  if count == 0:\n","    print(f'No corrupted files in {subset}:)')\n"],"metadata":{"id":"px5iO5W04cYk"},"id":"px5iO5W04cYk","execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["8pt0ofUcxZ73","fdcdd57c-b33f-4541-9cfd-3fb78b6ac1df","D3o60sb1vqun"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}